dev.off()
bestlambda <- cvr3$lambda.min
bestlambda
round(predict(r3, type = "coefficients", s = bestlambda), 3)
#### lasso
library(glmnet)
r4 <- glmnet(X, Y, alpha = 1)
r4$lambda[40]
coef(r4)[,40]
coef(r4)[,25]
set.seed(1234)
cvr4 <- cv.glmnet(X, Y, alpha = 1)
plot(cvr4)
dev.copy(pdf, "Fig2_5.pdf")
dev.off()
bestlambda <- cvr4$lambda.min
bestlambda
round(predict(r4, type = "coefficients", s = bestlambda), 3)
#### principal components regression
library(pls)
r5 <- pcr(y ~., data = dat, scale = T, validation = "CV")
install.packages("pls")
install.packages("tree")
install.packages("randomForest")
set.seed(1234)
x1 <- sample(1:10, 100, replace = T)
x2 <- sample(0:1, 100, replace = T)
x3 <- sample(0:3, 100, replace = T)
x4 <- sample(1:100, replace = T)
x5 <- sample(5:15, 100, replace = T)
x6 <- sample(c(10,20), 100, replace = T)
hy <- - 75 - 2*x1 + 5*x2 + 50*x3 - .5*x4 - x5 + 3*x6 + rnorm(100,0,10)
hist(hy)
y <- as.factor(hy > 0)  # necessary if one later wants to do classification
table(y)
dat <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5, x6 = x6)
train <- 1:70
###### Logit
r1 <- glm(y ~., data = dat[train,], family = binomial(link = "logit"))
summary(r1)
hpr1 <- predict(r1, newdata = dat[-train,])
pr1 <- (hpr1 >.5)
table(dat$y[-train], pr1)
###### CART
library(tree)
r2 <- tree(y ~., data = dat[train,])
summary(r2)
plot(r2)
text(r2)
dev.copy(pdf, "Fig3_1.pdf")
dev.off()
r2
pr2 <- predict(r2, newdata = dat[-train,], type = "class")
table(dat$y[-train], pr2)
table(pr1, pr2)
#### pruning
set.seed(1234)
cvr2 <- cv.tree(r2)
cvr2
plot(cvr2$size, cvr2$dev, type = "b",
xlab = "number of nodes", ylab = "impurity")
dev.copy(pdf, "Fig3_3.pdf")
dev.off()
prr2 <- prune.misclass(r2, best = 2)
plot(prr2)
text(prr2)
#### bagging
library(randomForest)
set.seed(1234)
bgr2 <- randomForest(y ~., data = dat, subset = train, mtry = 6, importance = T)
# mtry = 6: all predictors are used, i.e. bagging is performed
bgr2
prbgr2 <- predict(bgr2, newdata = dat[-train,])
table(dat$y[-train], prbgr2)
bgr2 <- randomForest(y ~., data = dat, subset = train, mtry = 6, importance = T)
# mtry = 6: all predictors are used, i.e. bagging is performed
bgr2
prbgr2 <- predict(bgr2, newdata = dat[-train,])
table(dat$y[-train], prbgr2)
#### boosting
library(randomForest)
set.seed(1234)
bsr2 <- randomForest(y ~., data = dat, subset = train, importance = T)
prbsr2 <- predict(bsr2, newdata = dat[-train,])
table(dat$y[-train], prbsr2)
importance(bsr2)
varImpPlot(bsr2)
rm(list = ls())
graphics.off()
###### MACHINE LEARNING
###### NAIVE BAYES
##### simulated data
set.seed(1234)
x1 <- sample(0:1, 300, replace = T)
x2 <- sample(0:1, 300, replace = T)
x3 <- sample(0:1, 300, replace = T)
x4 <- sample(0:1, 300, replace = T)
x5 <- sample(0:1, 300, replace = T)
e <- rnorm(300,0,1)
hy <- 3 + x1 + 6*x2 + .5*x1*x2 - 10*x3 + 5*x4 + .8*x3*x4 + 6*x5 + e
hist(hy)
y = as.numeric(hy > 12)
dat <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5)
train <- 1:200
##### OLS (linear probability)
r1 <- lm(y ~., data = dat[train,])
summary(r1)
prdr1 <- as.numeric(predict(r1, newdata = dat[-train,]) > .5)
table(dat$y[-train],prdr1)
##### logit
l1 <- glm(y ~ ., data = dat[train,], family = binomial(link = "logit"))
summary(l1)
prdl1 <- as.numeric(predict.glm(l1, newdata = dat[-train,]) > 0)
table(dat$y[-train], prdl1)
l2 <- glm(y ~ x1 + x2 + x3 + x4 + x5 + x1*x2 + x2*x3 + x3*x4 + x4*x5,
data = dat[train,], family = binomial(link = "logit"))
summary(l2)
prdl2 <- as.numeric(predict.glm(l2, newdata = dat[-train,]) > 0)
table(dat$y[-train], prdl2)
##### Naive Bayes
library(e1071)
install.packages("e1071")
nb1 <- naiveBayes(y ~., data = dat[train,], laplace = 0)
hprnb1 <- predict(nb1, newdata = dat[-train,], type = "raw")
##### Naive Bayes
library(e1071)
nb1 <- naiveBayes(y ~., data = dat[train,], laplace = 0)
hprnb1 <- predict(nb1, newdata = dat[-train,], type = "raw")
prnb1 <- as.numeric(hprnb1[,2] > .5)
table(dat$y[-train], prnb1)
# with Laplace correction
nb2 <- naiveBayes(y ~., data = dat[train,], laplace = 1)
hprnb2 <- predict(nb1, newdata = dat[-train,], type = "raw")
prnb2 <- as.numeric(hprnb2[,2] > .5)
table(dat$y[-train], prnb2)
set.seed(1234)
phi <- runif(100,0,2*pi)
rm(list = ls())
graphics.off()
set.seed(1234)
phi <- runif(100,0,2*pi)
x1 <- 3 + 2 * cos(phi) + rnorm(100,0,.5)
y1 <- 5 + 2 * sin(phi) + rnorm(100,0,.5)
plot(x, y)
x2 <- 3 + 4 * cos(phi) + rnorm(100,0,.5)
y2 <- 5 + 4 * sin(phi) + rnorm(100,0,.5)
plot(x2, y2, pch = 17, col = 3)
plot(x, y)
plot(x1, y1)
install.packages("neuralnet")
dat <- data.frame(x = x1, y = y1, type = 0)
dat2 <- data.frame(x = x2, y = y2, type = 1)
dat <- rbind(dat, dat2)
names(dat) <- c("x1", "x2", "y")
dat$y <- as.factor(dat$y)
train <- sample(1:200,140,replace = F)
library(ggplot2)
ggplot(data= dat, aes(x=x1, y=x2, color = y)) +
geom_point(pch = 17, cex = 2) +
theme(panel.background = element_blank()) +
theme_linedraw()
dev.copy(pdf, "Fig3_4.pdf")
dev.off()
##### logit
r1 <- glm(y ~., data = dat, subset = train, family = binomial(link = "logit"))
summary(r1)
hprr1 <- predict(r1, newdata = dat[-train,])
prr1 <- hprr1 > .5
table(dat$y[-train],prr1)
r2 <- glm(y ~ poly(x1, 3) + poly(x2, 3), data = dat, subset = train,
family = binomial(link = "logit"))
summary(r2)
hprr2 <- predict(r2, newdata = dat[-train,])
prr2 <- hprr2 > .5
table(dat$y[-train], prr2)
##### support vector classifier
library(e1071)
r3 <- svm(y ~., data = dat, subset = train, kernel = "linear", cost = 10, scale = F)
summary(r3)
plot(r3, dat)
prr3 <- predict(r3, newdata = dat[-train,])
table(true = dat$y[-train], pred = prr3)
##### support vector machine
r4 <- svm(y ~., data = dat, subset = train, kernel = "radial", cost = 10, scale = F)
summary(r4)
plot(r4, dat)
dev.copy(pdf, "Fig3_6.pdf")
dev.off()
prr4 <- predict(r4, newdata = dat[-train,])
table(true = dat$y[-train], pred = prr4)
#### fine tune cost parameter
# use all data for finding the most appropriate parameter
tnr4 <- tune(svm, y ~., data = dat, kernel = "radial",
ranges = list(cost = c(.001, .01, .1, 1, 10, 100)))
summary(tnr4)
# same for gamma (conditional on optimal cost)
tnr4 <- tune(svm, y ~., data = dat, kernel = "radial", cost = 10,
ranges = list(gamma = c(.001, .01, .1, 1, 10, 100)))
summary(tnr4)
# simultaneously varying cost and gamma
tnr4 <- tune(svm, y ~., data = dat, kernel = "radial",
ranges = list(cost = c(.001, .01, .1, 1, 10, 100),
gamma = c(.001, .01, .1, 1, 10, 100)))
summary(tnr4)
plot(tnr4, xlim = c(.001,1.5), ylim = c(.001, 1.5))
dev.copy(pdf, "Fig3_7.pdf")
dev.off()
rm(list = ls())
graphics.off()
###### MACHINE LEARNING
###### NEURAL NETWORK
###### simulated data
set.seed(1234)
#### (observed) indicator variables
x1 <- sample(0:1, 1000, replace = T)
x2 <- sample(0:5, 1000, replace = T)
x3 <- sample(0:10, 1000, replace = T)
x4 <- sample(0:1, 1000, replace = T)
x5 <- sample(0:15, 1000, replace = T)
x6 <- sample(0:2, 1000, replace = T)
x7 <- sample(0:1, 1000, replace = T)
x8 <- sample(0:5, 1000, replace = T)
x9 <- sample(0:1, 1000, replace = T)
x10 <- sample(0:3, 1000, replace = T)
#### level 1 latent variables
e1 <- rnorm(1000)
e2 <- rnorm(1000,0,2)
e3 <- rnorm(1000,0,3)
e4 <- rnorm(1000)
e5 <- rnorm(1000,0,2)
l1 <- 2 - x1 + 2*x2 - 3*x3 + 5*x10 + e1
l2 <- -5 - 2*x4 + 3*x5 + 5*x8 - 2*x10 + e2
l3 <- 10 + 2*x2 + 3*x4 + 5*x6 - 3*x7 - 4*x8 + e3
l4 <- -20 + 2*x1 - 3*x3 + 5*x8 + 2*x9 - x10 + e4
l5 <- 5 + 2*x4 + 4*x6 - 6*x8 + 8*x10 + e5
#### level 2 latent variables
e6 <- rnorm(1000,0,2)
e7 <- rnorm(1000,0,3)
e8 <- rnorm(1000)
e9 <- rnorm(1000)
e10 <- rnorm(1000,0,2)
m1 <- 10 - 2*l1 + 4*l2 - 3*l3 + e6
m2 <- -5 + 5*l2 - 2*l3 + 4*l5 + e7
m3 <- -2 - 2*l1 + 2*l4 + 2*l5 + e8
m4 <- 5 + 2*l1 + 2*l2 + 4*l4 - l5 + e9
m5 <- 3 + 2*l2 + 4*l3 + 2*l4 + e10
#### (observed) dependent variable
e11 <- rnorm(1000)
y <- 5 + 2*m1 + 3*m2 - 2*m3 - 4*m4 + m5 + e11
#### dataset (of observed variables)
dat <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5,
x6 = x6, x7 = x7, x8 = x8, x9 = x9, x10 = x10)
train <- dat[1:700,]
test <- dat[701:1000,]
#### normalized variables
scdat <- data.frame(scale(dat))
sctrain <- scdat[1:700,]
sctest <- scdat[701:1000,]
#### descriptives
hist(train$y)
hist(sctrain$y)
##### OLS (on original data)
r1 <- lm(y ~., data = train)
summary(r1)
prdr1 <- predict(r1, newdata = test)
prdat <- cbind(test, prdr1)
prdat$sqdiff <- (prdat$y - prdat$prdr1)^2
sqrt(sum(prdat$sqdiff))
##### OLS (on normalized data)
r2 <- lm(y ~., data = sctrain)
summary(r2)
prdr2 <- predict(r2, newdata = sctest)
prdat <- cbind(sctest, prdr2)
prdat$sqdiff <- (prdat$y - prdat$prdr2)^2
sqrt(sum(prdat$sqdiff))
##### neural network
#### one hidden layer, no normalization
library(neuralnet)
n1 <- neuralnet(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = train, hidden = 1)
prdn1 <- compute(n1, test[,-1])  # must remove predictor variable from test data
summary(prdn1$net.result)
#### one hidden layer, all variables normalized
n2 <- neuralnet(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = sctrain, hidden = 1, stepmax = 1e+06)
plot(n2, dimension = 6)
dev.copy(pdf, "Fig3_8.pdf")
dev.off()
# does not converge with default stepmax = 1e+05
prdn2 <- compute(n2, sctest[,-1])
hist(prdn2$net.result)
prdat <- data.frame(sctest, pred = prdn2$net.result)
prdat$sqdiff <- (prdat$y - prdat$pred)^2
sqrt(sum(prdat$sqdiff))
#### five hidden layers, normalization
n3 <- neuralnet(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = sctrain, hidden = 5, stepmax = 1e+06)
plot(n3)
dev.copy(pdf, "Fig3_9.pdf")
dev.off()
prdn3 <- compute(n3, sctest[,-1])
prdat <- data.frame(sctest, pred = prdn3$net.result)
prdat$sqdiff <- (prdat$y - prdat$pred)^2
sqrt(sum(prdat$sqdiff))
plot(n3)
#### five hidden layers, normalization
n3 <- neuralnet(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10,
data = sctrain, hidden = 5, stepmax = 1e+06)
plot(n3)
dev.copy(pdf, "Fig3_9.pdf")
dev.off()
prdn3 <- compute(n3, sctest[,-1])
prdat <- data.frame(sctest, pred = prdn3$net.result)
prdat$sqdiff <- (prdat$y - prdat$pred)^2
sqrt(sum(prdat$sqdiff))
rm(list = ls())
rm(list = ls())
graphics.off()
set.seed(1234)
x1 <- sample(0:1, 1000, replace = T, prob = c(.8,.2))
x2 <- sample(0:1, 1000, replace = T, prob = c(.9,.1))
x3 <- sample(0:1, 1000, replace = T)
x4 <- sample(0:1, 1000, replace = T)
x5 <- sample(0:1, 1000, replace = T)
x6 <- sample(0:1, 1000, replace = T)
hy <- 2 - 2*x1 + 5*x2 + 2*x3 - x4 - 3*x5 + 3*x6 + rnorm(1000,0,5)
hist(hy)
y <- as.factor(hy > 0)  # necessary if one later wants to do classification
table(y)
dat <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4, x5 = x5, x6 = x6)
train <- 1:700
##### CART
### single run, no boosting
set.seed(1234)
library(tree)
cr1 <- tree(y ~ x1 + x2 + factor(x3) + x4 + x5 + factor(x6), data = dat[train,])
cr2 <- tree(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = dat[train,])
View(cr1)
cr1 = cr2
cr1 <- tree(y ~ x1 + x2 + factor(x3) + x4 + x5 + factor(x6), data = dat[train,])
cr2 <- tree(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = dat[train,])
identical
identical(cr1, cr2)
prcr1 <- predict(cr1, newdata = dat[-train,], type = "class")
cm1 <- table(dat$y[-train], prcr1)
cm1
(cm1[1,2] + cm1[2,1])/300
### with boosting
library(randomForest)
set.seed(1234)
# boosting is in the command randomForest
cr2 <- randomForest(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = dat,
subset = train, importance = T)
# we have unbalanced data points. It means some IVs are more important than the others.
# boosting should kick out the unimportant IVs.
prcr2 <- predict(cr2, newdata = dat[-train,])
cm2 <- table(dat$y[-train], prcr2)
cm2
(cm2[1,2] + cm2[2,1])/300
##### Naive Bayes w/o Laplace correction
summary(dat)
library(e1071)
nb1 <- naiveBayes(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = dat[train,],
laplace = 0)
hprnb1 <- predict(nb1, newdata = dat[-train,], type = "raw")
prnb1 <- as.numeric(hprnb1[,2] > .5)
cm3<- table(dat$y[-train], prnb1)
cm3
(cm3[1,2] + cm3[2,1])/300
##### Naive Bayes w Laplace correction
nb2 <- naiveBayes(y ~ x1 + x2 + factor(x3) + x4 + x5 + factor(x6), data = dat[train,],
laplace = 1)
hprnb1 <- predict(nb2, newdata = dat[-train,], type = "raw")
prnb2 <- as.numeric(hprnb1[,2] > .5)
cm4 <- table(dat$y[-train], prnb2)
(cm4[1,2] + cm4[2,1])/300
log <- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = dat[train,],
family = binomial(link = "logit"))
summary(log)
prlog <- (predict.glm(log, newdata = dat[-train,], type = "response") > .5)
hist(prlog)
prlog <- as.numeric(prlog)
hist(prlog)
cm5 <- table(dat$y[-train], prlog)
cm5
(cm5[1,2] + cm5[2,1])/300
install.packages("ptw")
install.packages("pwt")
install.packages("plotrix")
install.packages("lattice")
install.packages("Hmisc")
install.packages("latticeExtra")
install.packages("vcd")
install.packages("MASS")
install.packages("reshape2")
install.packages("ks")
install.packages("Ecdat")
install.packages("mgcv")
install.packages("ggplot2")
install.packages("ggthemes")
install.packages("R2jags")
rawdata_classroom_conditionA <- read.csv("~/Dropbox/01 PhD/09 Confidence/Study1/Experiment/Otree data/parameter estimation/Jags/rawdata_classroom_conditionA.csv", header=FALSE)
View(rawdata_classroom_conditionA)
rawdata_classroom_conditionA <- read.csv("~/Dropbox/01 PhD/09 Confidence/Study1/Experiment/Otree data/parameter estimation/Jags/rawdata_classroom_conditionA.csv")
View(rawdata_classroom_conditionA)
library(haven)
study1b <- read_dta("Dropbox/01 PhD/09 Confidence/Study1/Experiment/Otree data/01 Classroom/DataAnalysis/study1b.dta")
View(study1b)
df <- data.frame(study1b)
df$sessioncode == "8aeenjqb"
sum(df$sessioncode == "8aeenjqb")
sum(df$sessioncode == "0h61stw2")
sum(df$sessioncode == "v9cmd6if")
sum(df$sessioncode == "9jnf5pxn")
sum(df$sessioncode == "mm2xiwnc")
sum(df$sessioncode == "836y81y6")
library(haven)
study1c <- read_dta("Dropbox/01 PhD/09 Confidence/Study1/Experiment/Otree data/01 Classroom/DataAnalysis/study1c.dta")
View(study1c)
df <- data.frame(study1c)
sum(df$sessioncode == "8aeenjqb")
sum(df$sessioncode == "0h61stw2")
sum(df$sessioncode == "v9cmd6if")
sum(df$sessioncode == "9jnf5pxn")
sum(df$sessioncode == "836y81y6")
sum(df$sessioncode == "mm2xiwnc")
library(haven)
study1c_yoked <- read_dta("Dropbox/01 PhD/09 Confidence/Study1/Experiment/Otree data/01 Classroom/DataAnalysis/study1c_yoked.dta")
View(study1c_yoked)
df <- data.frame(study1c)
library(haven)
study1c <- read_dta("Dropbox/01 PhD/09 Confidence/Study1/Experiment/Otree data/01 Classroom/DataAnalysis/study1c.dta")
View(study1c)
df <- data.frame(study1c)
df1 <- data.frame(study1c_yoked)
a <- df$participantcode
b <- df1$participantcode
setdiff(b,a)
unique(a[! a %in% b])
setwd("~/otreenew/yoking")
yoking <- read.csv(file = 'yoking.csv')
# keep 3 rows only
myData <- yoking[-c(1, 2, 3), ]
# keep 3 rows only
myData <- yoking[c(1, 2, 3), ]
write.csv(myData,'yokingnew.csv')
View(myData)
View(myData)
mydata1 = select(starts_with("amount"))
library(tidyverse)
install.packages("library(tidyverse)")
install.packages("tidyverse")
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
setwd("~/otreenew/yoking")
> library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
> library("dplyr")
library("dplyr")
yoking <- read.csv(file = 'yoking.csv')
# keep 3 rows only
myData <- yoking[c(1, 2, 3), ]
mydata1 = select(starts_with("amount"))
mydata1 = select(myDate, starts_with("amount"))
mydata1 = select(myData, starts_with("amount"))
View(mydata1)
View(mydata1)
View(myData)
View(myData)
mydata1 = select(myData, starts_with("player.amount"))
View(myData)
View(myData)
View(mydata1)
View(mydata1)
mydata1 = select(myData, starts_with("player.amounta"))
mydata1 = select(myData, starts_with("participant.code"))
mydata2 = select(myData, starts_with("player.bo"))
setwd("~/otreenew/yoking")
library("dplyr")
yoking <- read.csv(file = 'yoking.csv')
# keep 3 rows only
myData <- yoking[c(1, 2, 3), ]
mydata1 = select(myData, starts_with("participant.code"))
mydata2 = select(myData, starts_with("player.bo"))
mydata3 = select(myData, starts_with("player.index"))
mydata4 = select(myData, starts_with("player.trivial"))
mydata5 = select(myData, starts_with("player.realamount"))
mydata6 = select(myData, starts_with("player.realsample"))
mydata7 = select(myData, starts_with("player.realproba"))
mydata8 = select(myData, starts_with("player.realprobb"))
df = data.frame(mydata1,mydata2,mydata3,mydata4,mydata5,mydata6,mydata7,mydata8)
View(df)
mydata2 = select(myData, starts_with("participant.code","player.bo"))
View(mydata2)
View(mydata2)
View(df)
View(df)
write.csv(myData,'yokingnew.csv')
write.csv(myData,'yokingnew1.csv')
setwd("~/otreenew/yoking")
library("dplyr")
yoking <- read.csv(file = 'yoking.csv')
# keep 3 rows only
myData <- yoking[c(1, 2, 3), ]
mydata1 = select(myData, starts_with("participant.code"))
mydata2 = select(myData, starts_with("player.bo"))
mydata3 = select(myData, starts_with("player.index"))
mydata4 = select(myData, starts_with("player.trivial"))
mydata5 = select(myData, starts_with("player.realamount"))
mydata6 = select(myData, starts_with("player.realsample"))
mydata7 = select(myData, starts_with("player.realproba"))
mydata8 = select(myData, starts_with("player.realprobb"))
df = data.frame(mydata1,mydata2,mydata3,mydata4,mydata5,mydata6,mydata7,mydata8)
write.csv(df,'yokingnew.csv')
